{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entity Resolution- Dedupe Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import Statements\n",
    "from __future__ import print_function\n",
    "from future.builtins import next\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import re\n",
    "import collections\n",
    "import numpy\n",
    "\n",
    "import dedupe\n",
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the output_file will store the results of the record linkage deduplication\n",
    "output_file = 'data_matching_output.csv'\n",
    "\n",
    "# the settings file will contain the data model and predicates that determine matches\n",
    "settings_file = 'data_matching_learned_settings'\n",
    "\n",
    "# the training_file will contain the pairs of labeled examples that the model was trained on\n",
    "training_file = 'data_matching_training.json'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# method to clean the data using Unidecode and Regex\n",
    "\n",
    "def preProcess(column):\n",
    "    # convert any unicode data into ASCII characters\n",
    "    column = unidecode(column)\n",
    "    # ignore new lines\n",
    "    column = re.sub('\\n', ' ', column)\n",
    "    # ignore special characters\n",
    "    column = re.sub('-', '', column)\n",
    "    column = re.sub('/', ' ', column)\n",
    "    column = re.sub(\"'\", '', column)\n",
    "    column = re.sub(\",\", '', column)\n",
    "    column = re.sub(\":\", ' ', column)\n",
    "    # ignore extra white space\n",
    "    column = re.sub('  +', ' ', column)\n",
    "    # ignore casing\n",
    "    column = column.strip().strip('\"').strip(\"'\").lower().strip()\n",
    "    if not column :\n",
    "        column = None\n",
    "    return column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read the data from the CSV and create a dictionary of addresses\n",
    "\n",
    "def readData(filename):\n",
    "    \n",
    "    # initialize the dictionary\n",
    "    data_d = {}\n",
    "\n",
    "    # read each row in the CSV, clean the data, and it to a dictionary\n",
    "    with open(filename) as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for i, row in enumerate(reader):\n",
    "            clean_row = dict([(k, preProcess(v)) for (k, v) in row.items()])\n",
    "            # each address will have a unique ID consisting of the file name and a unique number\n",
    "            data_d[filename + str(i)] = dict(clean_row)\n",
    "\n",
    "    return data_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the data\n",
    "data_1 = readData('companies_final.csv')\n",
    "data_2 = readData('companies_final_users.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:dedupe.api:((SimplePredicate: (sameFiveCharStartPredicate, name), SimplePredicate: (tokenFieldPredicate, ctry)), (SimplePredicate: (firstTokenPredicate, addr), SimplePredicate: (hundredIntegerPredicate, name)), (LevenshteinSearchPredicate: (4, name), SimplePredicate: (alphaNumericPredicate, addr)), (SimplePredicate: (commonThreeTokens, name), SimplePredicate: (tokenFieldPredicate, ctry)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading from data_matching_learned_settings\n"
     ]
    }
   ],
   "source": [
    "# if there already exists a settings_file (with the data model),\n",
    "# create a Dedupe object (linker) that will load the saved settings\n",
    "if os.path.exists(settings_file):\n",
    "    print('reading from', settings_file)\n",
    "    with open(settings_file, 'rb') as sf :\n",
    "        # create a record link object for saved settings- pass the data model to it\n",
    "        linker = dedupe.StaticRecordLink(sf)\n",
    "\n",
    "# if there is no previously saved settings data, create it\n",
    "else:\n",
    "    # Define the fields the linker will pay attention to\n",
    "    # Specifying fields refines the comparison methods so not each part of the record are compared equally\n",
    "    # for example, dedupe will learn which of these fields have higher weights (more important in determining matches) by using regularized logistic regression\n",
    "    # String types compared using affine gap string distance\n",
    "    # Note- the address field is only for US addresses (uses usaddress package to split into components)\n",
    "        # must have dedupe-variable-address installed to use this\n",
    "    fields = [\n",
    "        {'field' : 'name', 'type': 'String'},\n",
    "        {'field' : 'addr', 'type': 'String'},\n",
    "        {'field' : 'addr', 'type': 'Address'},\n",
    "        {'field' : 'city', 'type': 'String'},\n",
    "        {'field' : 'ctry', 'type': 'String'},\n",
    "        {'field' : 'code', 'type': 'String'}\n",
    "    ]\n",
    "    \n",
    "    # Create a new linker object\n",
    "    linker = dedupe.RecordLink(fields)\n",
    "    # To train the linker, feed it a sample of records.\n",
    "    linker.sample(data_1, data_2, 15000)\n",
    "\n",
    "    \n",
    "    # If we have training data saved from a previous run of linker,\n",
    "    # look for it and load it in.\n",
    "    if os.path.exists(training_file):\n",
    "        print('reading labeled examples from ', training_file)\n",
    "        with open(training_file) as tf :\n",
    "            linker.readTraining(tf)\n",
    "\n",
    "    # ## Active learning\n",
    "    # Dedupe will find the next pair of records\n",
    "    # it is least certain about and ask for them to be labelled as matches\n",
    "    # or not.\n",
    "    print('starting active labeling...')\n",
    "\n",
    "    # Label examples, add them to the training data, and update the mathcing model\n",
    "    dedupe.consoleLabel(linker)\n",
    "\n",
    "    linker.train()\n",
    "\n",
    "    # When finished, save training data as labeled examples in the training_file\n",
    "    with open(training_file, 'w') as tf :\n",
    "        linker.writeTraining(tf)\n",
    "\n",
    "    # Save weights and predicates.  If the settings file\n",
    "    # exists, skip all the training and learning next time we run\n",
    "    # this file.\n",
    "    with open(settings_file, 'wb') as sf :\n",
    "        linker.writeSettings(sf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Blocking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "# Split the data up into groups of records with some feature in common\n",
    "# Only comparing the entries in these blocks reduces number of comparisons\n",
    "# This is more useful with larger datasets- where we would use a representative sample rather than all the data\n",
    "# 2 blocking methods- predicate blocks and index (using inverted index)\n",
    "blocks = linker._blockData(data_1,data_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate precision, recall, threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:dedupe.api:0 records\n",
      "/anaconda/lib/python3.6/site-packages/dedupe/backport.py:20: UserWarning: NumPy linked against 'Accelerate.framework'. Multiprocessing will be disabled. http://mail.scipy.org/pipermail/numpy-discussion/2012-August/063589.html\n",
      "  warnings.warn(\"NumPy linked against 'Accelerate.framework'. \"\n",
      "INFO:dedupe.api:100 records\n",
      "INFO:dedupe.api:200 records\n",
      "INFO:dedupe.api:300 records\n",
      "INFO:dedupe.api:400 records\n",
      "INFO:dedupe.api:500 records\n",
      "INFO:dedupe.api:600 records\n",
      "INFO:dedupe.api:700 records\n",
      "INFO:dedupe.api:800 records\n",
      "INFO:dedupe.api:900 records\n"
     ]
    }
   ],
   "source": [
    "# the records will be duplicates\n",
    "candidate_records = itertools.chain.from_iterable(linker._blockedPairs(blocks))\n",
    "\n",
    "# Calculate the probability that pair of records are duplicates\n",
    "probability = dedupe.core.scoreDuplicates(candidate_records,\n",
    "                                           linker.data_model,\n",
    "                                           linker.classifier,\n",
    "                                           linker.num_cores)['score']\n",
    "\n",
    "probability = probability.copy()\n",
    "probability.sort()\n",
    "probability = probability[::-1]\n",
    "\n",
    "expected_dupes = numpy.cumsum(probability)\n",
    "\n",
    "# Recall- TP/(TP + FN) - ability to find all interesting data points\n",
    "recall = expected_dupes / expected_dupes[-1]\n",
    "# Precision- TP/(TP + FP) - take the probability that the pairs are duplicates divided by the total number of duplicates\n",
    "precision = expected_dupes / numpy.arange(1, len(expected_dupes) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print(\"probability:\", probability)\n",
    "# print(\"expected duplicates:\", expected_dupes)\n",
    "# print(\"recall:\",recall)\n",
    "# print(\"precision:\",precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEWCAYAAAB42tAoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11424aa90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display the predicted precision-recall plot\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.step(recall, precision, color='b', alpha=.2, where='post')\n",
    "plt.fill_between(recall, precision, step='post', alpha=0.2,\n",
    "                 color='b')\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title('Precision-Recall curve')\n",
    "plt.show()\n",
    "\n",
    "# PR curve looks nearly perfect due to small amounts of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:dedupe.api:0 records\n",
      "INFO:dedupe.api:100 records\n",
      "INFO:dedupe.api:200 records\n",
      "INFO:dedupe.api:300 records\n",
      "INFO:dedupe.api:400 records\n",
      "INFO:dedupe.api:500 records\n",
      "INFO:dedupe.api:600 records\n",
      "INFO:dedupe.api:700 records\n",
      "INFO:dedupe.api:800 records\n",
      "INFO:dedupe.api:900 records\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum expected recall and precision\n",
      "recall: %2.3f 0.94235754\n",
      "precision: %2.3f 0.8905470313095465\n",
      "With threshold: %2.3f 0.5453662\n",
      "Threshold to maximize expected F score = 0.5453662\n",
      "clustering...\n"
     ]
    }
   ],
   "source": [
    "# Find the threshold that will maximize a weighted average of our\n",
    "# precision and recall (F Score) for a sample of data.  When we set the recall weight to 2, we are\n",
    "# saying we care twice as much about recall as we do precision.\n",
    "#\n",
    "# The weighted avg, or F-score = 2tp/(2tp + fp + fn)\n",
    "# \n",
    "# In this case, set the recall_weight to .5- saying we care twice as much about precision\n",
    "# as we do recall. When matching the data, set the threshold closer to 1 to raise precision\n",
    "# \n",
    "# This is called hierarchical clustering with centroid linkage\n",
    "# Example- A is related to B, C is related to B, so those would all be clustered with B as centroid\n",
    "# The threshold determines the minimum probability for a record to be related to the centroid\n",
    "\n",
    "recall_weight = .9\n",
    "\n",
    "score = recall * precision / (recall + recall_weight ** 2 * precision)\n",
    "\n",
    "i = numpy.argmax(score)\n",
    "\n",
    "print('Maximum expected recall and precision')\n",
    "print('recall: %2.3f', recall[i])\n",
    "print('precision: %2.3f', precision[i])\n",
    "print('With threshold: %2.3f', probability[i])\n",
    "\n",
    "calcThreshold= probability[i]\n",
    "\n",
    "print(\"Threshold to maximize expected F score =\", calcThreshold )\n",
    "\n",
    "\n",
    "print('clustering...')\n",
    "linked_records = linker.match(data_1, data_2, threshold=calcThreshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# duplicate sets 60\n",
      "[0.01560063 0.03071953 0.0453536  0.05954734 0.07330279 0.08660963\n",
      " 0.099535   0.11209509 0.12430516 0.13617966 0.1477322  0.15897573\n",
      " 0.16992247 0.18057944 0.19095385 0.2010533  0.21089952 0.22049921\n",
      " 0.22986172 0.23893639 0.24777406 0.25638667 0.26479333 0.27300178\n",
      " 0.28100818 0.28883109 0.29645145 0.30390341 0.31119129 0.31829804\n",
      " 0.32522887 0.33198275 0.338547   0.34497261 0.35125235 0.35739333\n",
      " 0.36336552 0.36919073 0.3748941  0.38048118 0.3859439  0.39128956\n",
      " 0.3965335  0.40166769 0.40666433 0.41152038 0.4162813  0.42093233\n",
      " 0.42544307 0.42978902 0.43391643 0.43795272 0.44190479 0.44578823\n",
      " 0.44958798 0.45331037 0.45682972 0.46018666 0.46343909 0.46662392\n",
      " 0.46975728 0.47276689 0.4756198  0.47834604 0.48098828 0.48336691\n",
      " 0.48568764 0.48787273 0.48982012 0.49165778 0.49333627 0.49484577\n",
      " 0.49625268 0.49759701 0.49878272 0.49994568 0.50103708 0.50204218\n",
      " 0.50301314 0.50373795 0.50414115 0.50442591 0.50436173 0.5038349\n",
      " 0.50312717 0.50228228 0.50141408 0.5002817  0.49914459 0.49768258\n",
      " 0.49584886 0.49397937 0.49200944 0.48996536 0.48769417 0.48528356\n",
      " 0.48284916 0.48032693 0.47783588 0.47520155 0.47250602 0.46976747\n",
      " 0.46703953 0.46434317 0.46167154 0.45903071 0.45637708 0.45374807\n",
      " 0.45111941 0.44851892 0.44594837]\n",
      "0.5044259116650741\n"
     ]
    }
   ],
   "source": [
    "print('# duplicate sets', len(linked_records))\n",
    "print(score)\n",
    "i = numpy.argmax(score)\n",
    "print(score[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write our original data back out to a CSV with a new column called \n",
    "# 'Cluster ID' which indicates which records refer to each other.\n",
    "\n",
    "cluster_membership = {}\n",
    "cluster_id = None\n",
    "for cluster_id, (cluster, score) in enumerate(linked_records):\n",
    "    for record_id in cluster:\n",
    "        cluster_membership[record_id] = (cluster_id, score)\n",
    "\n",
    "if cluster_id :\n",
    "    unique_id = cluster_id + 1\n",
    "else :\n",
    "    unique_id =0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(output_file, 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    \n",
    "    header_unwritten = True\n",
    "\n",
    "    for fileno, filename in enumerate(('companies_final.csv', 'companies_final_users.csv')) :\n",
    "        with open(filename) as f_input :\n",
    "            reader = csv.reader(f_input)\n",
    "\n",
    "            if header_unwritten :\n",
    "                heading_row = next(reader)\n",
    "                heading_row.insert(0, 'source file')\n",
    "                heading_row.insert(0, 'Link Score')\n",
    "                heading_row.insert(0, 'Cluster ID')\n",
    "                writer.writerow(heading_row)\n",
    "                header_unwritten = False\n",
    "            else :\n",
    "                next(reader)\n",
    "\n",
    "            for row_id, row in enumerate(reader):\n",
    "                cluster_details = cluster_membership.get(filename + str(row_id))\n",
    "                if cluster_details is None :\n",
    "                    cluster_id = unique_id\n",
    "                    unique_id += 1\n",
    "                    score = None\n",
    "                else :\n",
    "                    cluster_id, score = cluster_details\n",
    "                row.insert(0, fileno)\n",
    "                row.insert(0, score)\n",
    "                row.insert(0, cluster_id)\n",
    "                writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:dedupe.api:((SimplePredicate: (sameFiveCharStartPredicate, name), SimplePredicate: (tokenFieldPredicate, ctry)), (SimplePredicate: (firstTokenPredicate, addr), SimplePredicate: (hundredIntegerPredicate, name)), (LevenshteinSearchPredicate: (4, name), SimplePredicate: (alphaNumericPredicate, addr)), (SimplePredicate: (commonThreeTokens, name), SimplePredicate: (tokenFieldPredicate, ctry)))\n",
      "INFO:dedupe.api:0 records\n",
      "INFO:dedupe.api:100 records\n",
      "INFO:dedupe.api:200 records\n",
      "INFO:dedupe.api:300 records\n",
      "INFO:dedupe.api:400 records\n",
      "INFO:dedupe.api:500 records\n",
      "INFO:dedupe.api:600 records\n",
      "INFO:dedupe.api:700 records\n",
      "INFO:dedupe.api:800 records\n",
      "INFO:dedupe.api:900 records\n"
     ]
    }
   ],
   "source": [
    "name = \"MOBILE LIMITED\"\n",
    "addr = \"city road 30\"\n",
    "city = \"London\"\n",
    "ctry = \"UK\"\n",
    "code = \"EC1Y 2AB\"\n",
    "\n",
    "user_entry = name + \" \" + addr + \" \" + city + \" \" + ctry + \" \" + code\n",
    "\n",
    "# create a new file that will contain the user's entry\n",
    "user_input_file = 'user_input_file.csv'\n",
    "with open (user_input_file, 'w', newline='') as csvfile:\n",
    "    fieldnames = ['id', 'name', 'addr', 'city', 'ctry', 'code']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    writer.writerow({'id' : \"1\", 'name':name, 'addr': addr, 'city':city, 'ctry':ctry, 'code':code})\n",
    "csvfile.close()\n",
    "\n",
    "data_entry = readData(user_input_file)\n",
    "data_1 = readData(\"companies_final.csv\")\n",
    "\n",
    "# now, need to update the settings file and training file when user enters a new address - add this later\n",
    "\n",
    "with open('data_matching_learned_settings', 'rb') as sf :\n",
    "    linker = dedupe.StaticRecordLink(sf)\n",
    "try:\n",
    "    match = linker.match(data_1, data_entry)\n",
    "    id = int(data_1[match[0][0][0]]['id'])\n",
    "    with open('companies_final.csv', 'r') as my_file:\n",
    "        reader = csv.reader(my_file)\n",
    "        rows = list(reader)\n",
    "        response_dedupe = ((rows[id][1] + \" \" + rows[id][2] + \" \" + rows[id][3] + \" \" + rows[id][4] + \" \" + rows[id][5]))\n",
    "        my_file.close()\n",
    "except dedupe.core.BlockingError:\n",
    "    response_dedupe = (\"No matching address was found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:dedupe.api:((SimplePredicate: (sameFiveCharStartPredicate, name), SimplePredicate: (tokenFieldPredicate, ctry)), (SimplePredicate: (firstTokenPredicate, addr), SimplePredicate: (hundredIntegerPredicate, name)), (LevenshteinSearchPredicate: (4, name), SimplePredicate: (alphaNumericPredicate, addr)), (SimplePredicate: (commonThreeTokens, name), SimplePredicate: (tokenFieldPredicate, ctry)))\n",
      "INFO:dedupe.api:0 records\n",
      "INFO:dedupe.api:100 records\n",
      "INFO:dedupe.api:200 records\n",
      "INFO:dedupe.api:300 records\n",
      "INFO:dedupe.api:400 records\n",
      "INFO:dedupe.api:500 records\n",
      "INFO:dedupe.api:600 records\n",
      "INFO:dedupe.api:700 records\n",
      "INFO:dedupe.api:800 records\n",
      "INFO:dedupe.api:900 records\n"
     ]
    }
   ],
   "source": [
    "name = \"1 mobile limited\"\n",
    "addr = \"30 city road\"\n",
    "city = \"Lon\"\n",
    "ctry = \"\"\n",
    "code = \"\"\n",
    "\n",
    "user_entry = name + \" \" + addr + \" \" + city + \" \" + ctry + \" \" + code\n",
    "\n",
    "# create a new file that will contain the user's entry\n",
    "user_input_file = 'user_input_file.csv'\n",
    "with open (user_input_file, 'w', newline='') as csvfile:\n",
    "    fieldnames = ['id', 'name', 'addr', 'city', 'ctry', 'code']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    writer.writerow({'id' : \"1\", 'name':name, 'addr': addr, 'city':city, 'ctry':ctry, 'code':code})\n",
    "csvfile.close()\n",
    "\n",
    "data_entry = readData(user_input_file)\n",
    "data_1 = readData(\"companies_final.csv\")\n",
    "\n",
    "# now, need to update the settings file and training file when user enters a new address - add this later\n",
    "\n",
    "with open('data_matching_learned_settings', 'rb') as sf :\n",
    "    linker = dedupe.StaticRecordLink(sf)\n",
    "try:\n",
    "    match = linker.match(data_1, data_entry)\n",
    "    id = int(data_1[match[0][0][0]]['id'])\n",
    "    with open('companies_final.csv', 'r') as my_file:\n",
    "        reader = csv.reader(my_file)\n",
    "        rows = list(reader)\n",
    "        response_dedupe = ((rows[id][1] + \" \" + rows[id][2] + \" \" + rows[id][3] + \" \" + rows[id][4] + \" \" + rows[id][5]))\n",
    "        my_file.close()\n",
    "except dedupe.core.BlockingError:\n",
    "    response_dedupe = (\"No matching address was found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:dedupe.api:((SimplePredicate: (sameFiveCharStartPredicate, name), SimplePredicate: (tokenFieldPredicate, ctry)), (SimplePredicate: (firstTokenPredicate, addr), SimplePredicate: (hundredIntegerPredicate, name)), (LevenshteinSearchPredicate: (4, name), SimplePredicate: (alphaNumericPredicate, addr)), (SimplePredicate: (commonThreeTokens, name), SimplePredicate: (tokenFieldPredicate, ctry)))\n",
      "INFO:dedupe.api:0 records\n",
      "INFO:dedupe.api:100 records\n",
      "INFO:dedupe.api:200 records\n",
      "INFO:dedupe.api:300 records\n",
      "INFO:dedupe.api:400 records\n",
      "INFO:dedupe.api:500 records\n",
      "INFO:dedupe.api:600 records\n",
      "INFO:dedupe.api:700 records\n",
      "INFO:dedupe.api:800 records\n",
      "INFO:dedupe.api:900 records\n"
     ]
    }
   ],
   "source": [
    "name = \"1 mobile limited\"\n",
    "addr = \"30 city road\"\n",
    "city = \"London\"\n",
    "ctry = \"UNITED KINGDON\"\n",
    "code = \"\"\n",
    "\n",
    "user_entry = name + \" \" + addr + \" \" + city + \" \" + ctry + \" \" + code\n",
    "\n",
    "# create a new file that will contain the user's entry\n",
    "user_input_file = 'user_input_file.csv'\n",
    "with open (user_input_file, 'w', newline='') as csvfile:\n",
    "    fieldnames = ['id', 'name', 'addr', 'city', 'ctry', 'code']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    writer.writerow({'id' : \"1\", 'name':name, 'addr': addr, 'city':city, 'ctry':ctry, 'code':code})\n",
    "csvfile.close()\n",
    "\n",
    "data_entry = readData(user_input_file)\n",
    "data_1 = readData(\"companies_final.csv\")\n",
    "\n",
    "# now, need to update the settings file and training file when user enters a new address - add this later\n",
    "\n",
    "with open('data_matching_learned_settings', 'rb') as sf :\n",
    "    linker = dedupe.StaticRecordLink(sf)\n",
    "try:\n",
    "    match = linker.match(data_1, data_entry)\n",
    "    id = int(data_1[match[0][0][0]]['id'])\n",
    "    with open('companies_final.csv', 'r') as my_file:\n",
    "        reader = csv.reader(my_file)\n",
    "        rows = list(reader)\n",
    "        response_dedupe = ((rows[id][1] + \" \" + rows[id][2] + \" \" + rows[id][3] + \" \" + rows[id][4] + \" \" + rows[id][5]))\n",
    "        my_file.close()\n",
    "except dedupe.core.BlockingError:\n",
    "    response_dedupe = (\"No matching address was found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:dedupe.api:((SimplePredicate: (sameFiveCharStartPredicate, name), SimplePredicate: (tokenFieldPredicate, ctry)), (SimplePredicate: (firstTokenPredicate, addr), SimplePredicate: (hundredIntegerPredicate, name)), (LevenshteinSearchPredicate: (4, name), SimplePredicate: (alphaNumericPredicate, addr)), (SimplePredicate: (commonThreeTokens, name), SimplePredicate: (tokenFieldPredicate, ctry)))\n",
      "INFO:dedupe.api:0 records\n",
      "INFO:dedupe.api:100 records\n",
      "INFO:dedupe.api:200 records\n",
      "INFO:dedupe.api:300 records\n",
      "INFO:dedupe.api:400 records\n",
      "INFO:dedupe.api:500 records\n",
      "INFO:dedupe.api:600 records\n",
      "INFO:dedupe.api:700 records\n",
      "INFO:dedupe.api:800 records\n",
      "INFO:dedupe.api:900 records\n"
     ]
    }
   ],
   "source": [
    "name = \"1 MOBILE LTD\"\n",
    "addr = \"300 CITY RD\"\n",
    "city = \"LON\"\n",
    "ctry = \"UK\"\n",
    "code = \"\"\n",
    "\n",
    "user_entry = name + \" \" + addr + \" \" + city + \" \" + ctry + \" \" + code\n",
    "\n",
    "# create a new file that will contain the user's entry\n",
    "user_input_file = 'user_input_file.csv'\n",
    "with open (user_input_file, 'w', newline='') as csvfile:\n",
    "    fieldnames = ['id', 'name', 'addr', 'city', 'ctry', 'code']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    writer.writerow({'id' : \"1\", 'name':name, 'addr': addr, 'city':city, 'ctry':ctry, 'code':code})\n",
    "csvfile.close()\n",
    "\n",
    "data_entry = readData(user_input_file)\n",
    "data_1 = readData(\"companies_final.csv\")\n",
    "\n",
    "# now, need to update the settings file and training file when user enters a new address - add this later\n",
    "\n",
    "with open('data_matching_learned_settings', 'rb') as sf :\n",
    "    linker = dedupe.StaticRecordLink(sf)\n",
    "try:\n",
    "    match = linker.match(data_1, data_entry)\n",
    "    id = int(data_1[match[0][0][0]]['id'])\n",
    "    with open('companies_final.csv', 'r') as my_file:\n",
    "        reader = csv.reader(my_file)\n",
    "        rows = list(reader)\n",
    "        response_dedupe = ((rows[id][1] + \" \" + rows[id][2] + \" \" + rows[id][3] + \" \" + rows[id][4] + \" \" + rows[id][5]))\n",
    "        my_file.close()\n",
    "except dedupe.core.BlockingError:\n",
    "    response_dedupe = (\"No matching address was found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Query Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ML model creates the rules for determining a match. They are dependent on training data and are not predetermined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User entry: MOBILE LIMITED city road 30 London UK EC1Y 2AB\n",
      "Response: 1 MOBILE LIMITED 30 CITY ROAD LONDON UK EC1Y 2AB\n"
     ]
    }
   ],
   "source": [
    "print(\"User entry:\", user_entry)\n",
    "print(\"Response:\", response_dedupe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User entry: 1 mobile limited 30 city road Lon  \n",
      "Response: 1 MOBILE LIMITED 30 CITY ROAD LONDON UK EC1Y 2AB\n"
     ]
    }
   ],
   "source": [
    "print(\"User entry:\", user_entry)\n",
    "print(\"Response:\", response_dedupe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User entry: 1 mobile limited 30 city road London UNITED KINGDON \n",
      "Response: 1 MOBILE LIMITED 30 CITY ROAD LONDON UK EC1Y 2AB\n"
     ]
    }
   ],
   "source": [
    "print(\"User entry:\", user_entry)\n",
    "print(\"Response:\", response_dedupe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User entry: 1 MOBILE LTD 300 CITY RD LON UK \n",
      "Response: 1 MOBILE LIMITED 30 CITY ROAD LONDON UK EC1Y 2AB\n"
     ]
    }
   ],
   "source": [
    "print(\"User entry:\", user_entry)\n",
    "print(\"Response:\", response_dedupe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Time Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:dedupe.api:((SimplePredicate: (sameFiveCharStartPredicate, name), SimplePredicate: (tokenFieldPredicate, ctry)), (SimplePredicate: (firstTokenPredicate, addr), SimplePredicate: (hundredIntegerPredicate, name)), (LevenshteinSearchPredicate: (4, name), SimplePredicate: (alphaNumericPredicate, addr)), (SimplePredicate: (commonThreeTokens, name), SimplePredicate: (tokenFieldPredicate, ctry)))\n",
      "INFO:dedupe.api:0 records\n",
      "INFO:dedupe.api:100 records\n",
      "INFO:dedupe.api:200 records\n",
      "INFO:dedupe.api:300 records\n",
      "INFO:dedupe.api:400 records\n",
      "INFO:dedupe.api:500 records\n",
      "INFO:dedupe.api:600 records\n",
      "INFO:dedupe.api:700 records\n",
      "INFO:dedupe.api:800 records\n",
      "INFO:dedupe.api:900 records\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17363691329956055\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "name = \"1 MOBILE LIMITED\"\n",
    "addr = \"30 CITY ROAD\"\n",
    "city = \"LONDON\"\n",
    "ctry = \"\"\n",
    "code = \"\"\n",
    "\n",
    "user_entry = name + \" \" + addr + \" \" + city + \" \" + ctry + \" \" + code\n",
    "\n",
    "# create a new file that will contain the user's entry\n",
    "user_input_file = 'user_input_file.csv'\n",
    "with open (user_input_file, 'w', newline='') as csvfile:\n",
    "    fieldnames = ['id', 'name', 'addr', 'city', 'ctry', 'code']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    writer.writerow({'id' : \"1\", 'name':name, 'addr': addr, 'city':city, 'ctry':ctry, 'code':code})\n",
    "csvfile.close()\n",
    "\n",
    "data_entry = readData(user_input_file)\n",
    "data_1 = readData(\"companies_final.csv\")\n",
    "\n",
    "# now, need to update the settings file and training file when user enters a new address - add this later\n",
    "\n",
    "with open('data_matching_learned_settings', 'rb') as sf :\n",
    "    linker = dedupe.StaticRecordLink(sf)\n",
    "try:\n",
    "    match = linker.match(data_1, data_entry)\n",
    "    id = int(data_1[match[0][0][0]]['id'])\n",
    "    with open('companies_final.csv', 'r') as my_file:\n",
    "        reader = csv.reader(my_file)\n",
    "        rows = list(reader)\n",
    "        response_dedupe = ((rows[id][1] + \" \" + rows[id][2] + \" \" + rows[id][3] + \" \" + rows[id][4] + \" \" + rows[id][5]))\n",
    "        my_file.close()\n",
    "except dedupe.core.BlockingError:\n",
    "    response_dedupe = (\"No matching address was found!\")\n",
    "    \n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:dedupe.api:((SimplePredicate: (sameFiveCharStartPredicate, name), SimplePredicate: (tokenFieldPredicate, ctry)), (SimplePredicate: (firstTokenPredicate, addr), SimplePredicate: (hundredIntegerPredicate, name)), (LevenshteinSearchPredicate: (4, name), SimplePredicate: (alphaNumericPredicate, addr)), (SimplePredicate: (commonThreeTokens, name), SimplePredicate: (tokenFieldPredicate, ctry)))\n",
      "INFO:dedupe.api:0 records\n",
      "INFO:dedupe.api:100 records\n",
      "INFO:dedupe.api:200 records\n",
      "INFO:dedupe.api:300 records\n",
      "INFO:dedupe.api:400 records\n",
      "INFO:dedupe.api:500 records\n",
      "INFO:dedupe.api:600 records\n",
      "INFO:dedupe.api:700 records\n",
      "INFO:dedupe.api:800 records\n",
      "INFO:dedupe.api:900 records\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.20071983337402344\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "name = \"1 MOBILE LIMITED\"\n",
    "addr = \"30 CITY STREET\"\n",
    "city = \"LONDON\"\n",
    "ctry = \"\"\n",
    "code = \"\"\n",
    "\n",
    "user_entry = name + \" \" + addr + \" \" + city + \" \" + ctry + \" \" + code\n",
    "\n",
    "# create a new file that will contain the user's entry\n",
    "user_input_file = 'user_input_file.csv'\n",
    "with open (user_input_file, 'w', newline='') as csvfile:\n",
    "    fieldnames = ['id', 'name', 'addr', 'city', 'ctry', 'code']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    writer.writerow({'id' : \"1\", 'name':name, 'addr': addr, 'city':city, 'ctry':ctry, 'code':code})\n",
    "csvfile.close()\n",
    "\n",
    "data_entry = readData(user_input_file)\n",
    "data_1 = readData(\"companies_final.csv\")\n",
    "\n",
    "# now, need to update the settings file and training file when user enters a new address - add this later\n",
    "\n",
    "with open('data_matching_learned_settings', 'rb') as sf :\n",
    "    linker = dedupe.StaticRecordLink(sf)\n",
    "try:\n",
    "    match = linker.match(data_1, data_entry)\n",
    "    id = int(data_1[match[0][0][0]]['id'])\n",
    "    with open('companies_final.csv', 'r') as my_file:\n",
    "        reader = csv.reader(my_file)\n",
    "        rows = list(reader)\n",
    "        response_dedupe = ((rows[id][1] + \" \" + rows[id][2] + \" \" + rows[id][3] + \" \" + rows[id][4] + \" \" + rows[id][5]))\n",
    "        my_file.close()\n",
    "except dedupe.core.BlockingError:\n",
    "    response_dedupe = (\"No matching address was found!\")\n",
    "    \n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:dedupe.api:((SimplePredicate: (sameFiveCharStartPredicate, name), SimplePredicate: (tokenFieldPredicate, ctry)), (SimplePredicate: (firstTokenPredicate, addr), SimplePredicate: (hundredIntegerPredicate, name)), (LevenshteinSearchPredicate: (4, name), SimplePredicate: (alphaNumericPredicate, addr)), (SimplePredicate: (commonThreeTokens, name), SimplePredicate: (tokenFieldPredicate, ctry)))\n",
      "INFO:dedupe.api:0 records\n",
      "INFO:dedupe.api:100 records\n",
      "INFO:dedupe.api:200 records\n",
      "INFO:dedupe.api:300 records\n",
      "INFO:dedupe.api:400 records\n",
      "INFO:dedupe.api:500 records\n",
      "INFO:dedupe.api:600 records\n",
      "INFO:dedupe.api:700 records\n",
      "INFO:dedupe.api:800 records\n",
      "INFO:dedupe.api:900 records\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17553186416625977\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "name = \"1 mobil lim\"\n",
    "addr = \"300 city rd\"\n",
    "city = \"lon\"\n",
    "ctry = \"uk\"\n",
    "code = \"\"\n",
    "\n",
    "user_entry = name + \" \" + addr + \" \" + city + \" \" + ctry + \" \" + code\n",
    "\n",
    "# create a new file that will contain the user's entry\n",
    "user_input_file = 'user_input_file.csv'\n",
    "with open (user_input_file, 'w', newline='') as csvfile:\n",
    "    fieldnames = ['id', 'name', 'addr', 'city', 'ctry', 'code']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    writer.writerow({'id' : \"1\", 'name':name, 'addr': addr, 'city':city, 'ctry':ctry, 'code':code})\n",
    "csvfile.close()\n",
    "\n",
    "data_entry = readData(user_input_file)\n",
    "data_1 = readData(\"companies_final.csv\")\n",
    "\n",
    "# now, need to update the settings file and training file when user enters a new address - add this later\n",
    "\n",
    "with open('data_matching_learned_settings', 'rb') as sf :\n",
    "    linker = dedupe.StaticRecordLink(sf)\n",
    "try:\n",
    "    match = linker.match(data_1, data_entry)\n",
    "    id = int(data_1[match[0][0][0]]['id'])\n",
    "    with open('companies_final.csv', 'r') as my_file:\n",
    "        reader = csv.reader(my_file)\n",
    "        rows = list(reader)\n",
    "        response_dedupe = ((rows[id][1] + \" \" + rows[id][2] + \" \" + rows[id][3] + \" \" + rows[id][4] + \" \" + rows[id][5]))\n",
    "        my_file.close()\n",
    "except dedupe.core.BlockingError:\n",
    "    response_dedupe = (\"No matching address was found!\")\n",
    "    \n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
