{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entity Resolution- Dedupe Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import Statements\n",
    "from __future__ import print_function\n",
    "from future.builtins import next\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import re\n",
    "import collections\n",
    "import numpy\n",
    "\n",
    "import dedupe\n",
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the output_file will store the results of the record linkage deduplication\n",
    "output_file = 'data_matching_output.csv'\n",
    "\n",
    "# the settings file will contain the data model and predicates that determine matches\n",
    "settings_file = 'data_matching_learned_settings'\n",
    "\n",
    "# the training_file will contain the pairs of labeled examples that the model was trained on\n",
    "training_file = 'data_matching_training.json'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# method to clean the data using Unidecode and Regex\n",
    "\n",
    "def preProcess(column):\n",
    "    # convert any unicode data into ASCII characters\n",
    "    column = unidecode(column)\n",
    "    # ignore new lines\n",
    "    column = re.sub('\\n', ' ', column)\n",
    "    # ignore special characters\n",
    "    column = re.sub('-', '', column)\n",
    "    column = re.sub('/', ' ', column)\n",
    "    column = re.sub(\"'\", '', column)\n",
    "    column = re.sub(\",\", '', column)\n",
    "    column = re.sub(\":\", ' ', column)\n",
    "    # ignore extra white space\n",
    "    column = re.sub('  +', ' ', column)\n",
    "    # ignore casing\n",
    "    column = column.strip().strip('\"').strip(\"'\").lower().strip()\n",
    "    if not column :\n",
    "        column = None\n",
    "    return column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read the data from the CSV and create a dictionary of addresses\n",
    "\n",
    "def readData(filename):\n",
    "    \n",
    "    # initialize the dictionary\n",
    "    data_d = {}\n",
    "\n",
    "    # read each row in the CSV, clean the data, and it to a dictionary\n",
    "    with open(filename) as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for i, row in enumerate(reader):\n",
    "            clean_row = dict([(k, preProcess(v)) for (k, v) in row.items()])\n",
    "            # each address will have a unique ID consisting of the file name and a unique number\n",
    "            data_d[filename + str(i)] = dict(clean_row)\n",
    "\n",
    "    return data_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the data\n",
    "data_1 = readData('addresses.csv')\n",
    "data_2 = readData('user_entries.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:dedupe.api:((SimplePredicate: (sameThreeCharStartPredicate, description), TfidfNGramSearchPredicate: (0.2, title)), (PartialPredicate: (metaphoneToken, title, StreetName), TfidfNGramSearchPredicate: (0.2, description)), (PartialPredicate: (metaphoneToken, title, StreetName), SimplePredicate: (commonThreeTokens, title)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading from data_matching_learned_settings\n"
     ]
    }
   ],
   "source": [
    "# if there already exists a settings_file (with the data model),\n",
    "# create a Dedupe object (linker) that will load the saved settings\n",
    "if os.path.exists(settings_file):\n",
    "    print('reading from', settings_file)\n",
    "    with open(settings_file, 'rb') as sf :\n",
    "        # create a record link object for saved settings- pass the data model to it\n",
    "        linker = dedupe.StaticRecordLink(sf)\n",
    "\n",
    "# if there is no previously saved settings data, create it\n",
    "else:\n",
    "    # Define the fields the linker will pay attention to\n",
    "    # Specifying fields refines the comparison methods so not each part of the record are compared equally\n",
    "    # for example, dedupe will learn which of these fields have higher weights (more important in determining matches) by using regularized logistic regression\n",
    "    # String types compared using affine gap string distance\n",
    "    # Note- the address field is only for US addresses (uses usaddress package to split into components)\n",
    "        # must have dedupe-variable-address installed to use this\n",
    "    fields = [\n",
    "        {'field' : 'title', 'type': 'String'},\n",
    "        {'field' : 'title', 'type': 'Address'},\n",
    "        {'field' : 'description', 'type': 'String'},\n",
    "        {'field' : 'description', 'type': 'Address'}]\n",
    "    \n",
    "    # Create a new linker object\n",
    "    linker = dedupe.RecordLink(fields)\n",
    "    # To train the linker, feed it a sample of records.\n",
    "    linker.sample(data_1, data_2, 15000)\n",
    "\n",
    "    \n",
    "    # If we have training data saved from a previous run of linker,\n",
    "    # look for it and load it in.\n",
    "    if os.path.exists(training_file):\n",
    "        print('reading labeled examples from ', training_file)\n",
    "        with open(training_file) as tf :\n",
    "            linker.readTraining(tf)\n",
    "\n",
    "    # ## Active learning\n",
    "    # Dedupe will find the next pair of records\n",
    "    # it is least certain about and ask for them to be labelled as matches\n",
    "    # or not.\n",
    "    print('starting active labeling...')\n",
    "\n",
    "    # Label examples, add them to the training data, and update the mathcing model\n",
    "    dedupe.consoleLabel(linker)\n",
    "\n",
    "    linker.train()\n",
    "\n",
    "    # When finished, save training data as labeled examples in the training_file\n",
    "    with open(training_file, 'w') as tf :\n",
    "        linker.writeTraining(tf)\n",
    "\n",
    "    # Save weights and predicates.  If the settings file\n",
    "    # exists, skip all the training and learning next time we run\n",
    "    # this file.\n",
    "    with open(settings_file, 'wb') as sf :\n",
    "        linker.writeSettings(sf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Blocking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "# Split the data up into groups of records with some feature in common\n",
    "# Only comparing the entries in these blocks reduces number of comparisons\n",
    "# This is more useful with larger datasets- where we would use a representative sample rather than all the data\n",
    "# 2 blocking methods- predicate blocks and index (using inverted index)\n",
    "blocks = linker._blockData(data_1,data_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate precision, recall, threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:dedupe.api:0 records\n",
      "/anaconda/lib/python3.6/site-packages/dedupe/backport.py:20: UserWarning: NumPy linked against 'Accelerate.framework'. Multiprocessing will be disabled. http://mail.scipy.org/pipermail/numpy-discussion/2012-August/063589.html\n",
      "  warnings.warn(\"NumPy linked against 'Accelerate.framework'. \"\n",
      "INFO:dedupe.api:100 records\n",
      "INFO:dedupe.api:200 records\n"
     ]
    }
   ],
   "source": [
    "# the records will be duplicates\n",
    "candidate_records = itertools.chain.from_iterable(linker._blockedPairs(blocks))\n",
    "\n",
    "# Calculate the probability that pair of records are duplicates\n",
    "probability = dedupe.core.scoreDuplicates(candidate_records,\n",
    "                                           linker.data_model,\n",
    "                                           linker.classifier,\n",
    "                                           linker.num_cores)['score']\n",
    "\n",
    "probability = probability.copy()\n",
    "probability.sort()\n",
    "probability = probability[::-1]\n",
    "\n",
    "expected_dupes = numpy.cumsum(probability)\n",
    "\n",
    "# Recall- TP/(TP + FN) - ability to find all interesting data points\n",
    "recall = expected_dupes / expected_dupes[-1]\n",
    "# Precision- TP/(TP + FP) - take the probability that the pairs are duplicates divided by the total number of duplicates\n",
    "precision = expected_dupes / numpy.arange(1, len(expected_dupes) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probability: [9.9999678e-01 9.9999678e-01 9.9999678e-01 9.9999678e-01 9.9999678e-01\n",
      " 9.9999678e-01 9.9999678e-01 9.9999654e-01 9.9999613e-01 9.9999541e-01\n",
      " 9.9999529e-01 9.9999315e-01 9.9999213e-01 9.9999034e-01 9.9998856e-01\n",
      " 9.9998468e-01 9.9997830e-01 9.9997461e-01 9.9996018e-01 9.9994999e-01\n",
      " 9.9994898e-01 9.9994260e-01 9.9994248e-01 9.9993414e-01 9.9993271e-01\n",
      " 9.9993044e-01 9.9988478e-01 9.9988425e-01 9.9986899e-01 9.9984628e-01\n",
      " 9.9984193e-01 9.9982285e-01 9.9981815e-01 9.9975628e-01 9.9969292e-01\n",
      " 9.9967510e-01 9.9966443e-01 9.9966043e-01 9.9962455e-01 9.9961346e-01\n",
      " 9.9939525e-01 9.9918050e-01 9.9914211e-01 9.9912804e-01 9.9900293e-01\n",
      " 9.9860072e-01 9.9859989e-01 9.9834120e-01 9.9830270e-01 9.9828058e-01\n",
      " 9.9824435e-01 9.9811625e-01 9.9797779e-01 9.9788743e-01 9.9710727e-01\n",
      " 9.9660027e-01 9.9534625e-01 9.9454069e-01 9.9242800e-01 9.9193501e-01\n",
      " 9.9143577e-01 9.8709077e-01 9.8643100e-01 9.6498561e-01 9.5814884e-01\n",
      " 8.7067372e-01 7.6535785e-01 7.0547503e-01 6.1362571e-01 5.4559320e-01\n",
      " 4.7085747e-01 4.7051996e-01 3.8545513e-01 6.1636142e-02 4.0203150e-02\n",
      " 1.4922452e-02 1.4090022e-02 9.7226892e-03 9.6218549e-03 4.0904656e-03\n",
      " 5.2538293e-04 1.0675468e-05]\n",
      "expected duplicates: [ 0.9999968  1.9999936  2.9999905  3.9999871  4.999984   5.9999804\n",
      "  6.999977   7.999974   8.9999695  9.999965  10.99996   11.999953\n",
      " 12.999946  13.999936  14.999925  15.999909  16.999887  17.999863\n",
      " 18.999823  19.999773  20.999722  21.999664  22.999607  23.99954\n",
      " 24.999474  25.999405  26.99929   27.999174  28.999043  29.998888\n",
      " 30.99873   31.998552  32.99837   33.998127  34.997818  35.997494\n",
      " 36.997158  37.99682   38.996445  39.99606   40.995453  41.994633\n",
      " 42.993774  43.9929    44.991905  45.990505  46.989105  47.987446\n",
      " 48.98575   49.984028  50.982273  51.98039   52.978367  53.976254\n",
      " 54.973362  55.969963  56.96531   57.95985   58.95228   59.944214\n",
      " 60.93565   61.92274   62.909172  63.874157  64.832306  65.70298\n",
      " 66.46834   67.17381   67.78744   68.33303   68.80389   69.27441\n",
      " 69.65986   69.7215    69.7617    69.77663   69.79072   69.80044\n",
      " 69.81006   69.81415   69.814674  69.81468  ]\n",
      "recall: [0.01432359 0.02864718 0.04297077 0.05729435 0.07161794 0.08594153\n",
      " 0.10026512 0.1145887  0.12891227 0.14323585 0.15755941 0.17188294\n",
      " 0.18620647 0.20052996 0.21485344 0.22917686 0.24350017 0.25782347\n",
      " 0.27214652 0.28646943 0.30079234 0.31511515 0.32943797 0.34376064\n",
      " 0.35808334 0.37240598 0.38672796 0.40104994 0.4153717  0.4296931\n",
      " 0.4440145  0.45833558 0.4726566  0.48697674 0.501296   0.5156149\n",
      " 0.52993375 0.5442526  0.5585708  0.572889   0.5872039  0.60151577\n",
      " 0.61582714 0.6301382  0.6444476  0.6587512  0.67305475 0.6873546\n",
      " 0.70165396 0.71595293 0.73025143 0.7445481  0.75884277 0.77313614\n",
      " 0.78741837 0.8016933  0.8159503  0.8301957  0.8444109  0.85861903\n",
      " 0.87281996 0.8869587  0.901088   0.9149101  0.9286343  0.9411055\n",
      " 0.9520682  0.96217316 0.97096246 0.97877735 0.98552173 0.9922613\n",
      " 0.99778235 0.9986653  0.9992412  0.9994549  0.99965674 0.999796\n",
      " 0.9999338  0.9999924  0.9999999  1.        ]\n",
      "precision: [0.99999678 0.99999678 0.99999682 0.99999678 0.99999676 0.99999674\n",
      " 0.99999673 0.99999672 0.99999661 0.99999647 0.99999636 0.99999611\n",
      " 0.99999582 0.99999544 0.99999498 0.99999434 0.99999338 0.99999237\n",
      " 0.99999066 0.99998865 0.99998674 0.99998474 0.99998292 0.99998085\n",
      " 0.99997894 0.99997711 0.99997372 0.9999705  0.99996698 0.99996293\n",
      " 0.99995902 0.99995476 0.99995064 0.99994491 0.99993766 0.99993038\n",
      " 0.99992319 0.99991628 0.99990884 0.99990149 0.99988909 0.99987221\n",
      " 0.99985522 0.99983866 0.99982012 0.99979359 0.9997682  0.99973845\n",
      " 0.99970915 0.99968056 0.99965241 0.99962286 0.99959183 0.99956025\n",
      " 0.99951567 0.99946363 0.99939139 0.99930776 0.99919115 0.99907023\n",
      " 0.99894508 0.99875389 0.99855829 0.9980337  0.99742009 0.9954997\n",
      " 0.99206475 0.98785019 0.98242663 0.97618615 0.96906882 0.96214453\n",
      " 0.95424464 0.94218239 0.93015605 0.91811351 0.90637296 0.89487741\n",
      " 0.88367163 0.87267685 0.86190956 0.85139856]\n"
     ]
    }
   ],
   "source": [
    "print(\"probability:\", probability)\n",
    "print(\"expected duplicates:\", expected_dupes)\n",
    "print(\"recall:\",recall)\n",
    "print(\"precision:\",precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEWCAYAAAB42tAoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10a5cca90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display the predicted precision-recall plot\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.step(recall, precision, color='b', alpha=.2, where='post')\n",
    "plt.fill_between(recall, precision, step='post', alpha=0.2,\n",
    "                 color='b')\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title('Precision-Recall curve')\n",
    "plt.show()\n",
    "\n",
    "# PR curve looks nearly perfect due to small amounts of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:dedupe.api:0 records\n",
      "INFO:dedupe.api:100 records\n",
      "INFO:dedupe.api:200 records\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum expected recall and precision\n",
      "recall: %2.3f 0.9411055\n",
      "precision: %2.3f 0.9954996975985441\n",
      "With threshold: %2.3f 0.8706737\n",
      "Threshold to maximize expected F score = 0.8706737\n",
      "clustering...\n"
     ]
    }
   ],
   "source": [
    "# Find the threshold that will maximize a weighted average of our\n",
    "# precision and recall (F Score) for a sample of data.  When we set the recall weight to 2, we are\n",
    "# saying we care twice as much about recall as we do precision.\n",
    "#\n",
    "# The weighted avg, or F-score = 2tp/(2tp + fp + fn)\n",
    "# \n",
    "# In this case, set the recall_weight to .5- saying we care twice as much about precision\n",
    "# as we do recall. When matching the data, set the threshold closer to 1 to raise precision\n",
    "# \n",
    "# This is called hierarchical clustering with centroid linkage\n",
    "# Example- A is related to B, C is related to B, so those would all be clustered with B as centroid\n",
    "# The threshold determines the minimum probability for a record to be related to the centroid\n",
    "\n",
    "recall_weight = .5\n",
    "\n",
    "score = recall * precision / (recall + recall_weight ** 2 * precision)\n",
    "\n",
    "i = numpy.argmax(score)\n",
    "\n",
    "print('Maximum expected recall and precision')\n",
    "print('recall: %2.3f', recall[i])\n",
    "print('precision: %2.3f', precision[i])\n",
    "print('With threshold: %2.3f', probability[i])\n",
    "\n",
    "calcThreshold= probability[i]\n",
    "\n",
    "print(\"Threshold to maximize expected F score =\", calcThreshold )\n",
    "\n",
    "\n",
    "print('clustering...')\n",
    "linked_records = linker.match(data_1, data_2, threshold=calcThreshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# duplicate sets 36\n"
     ]
    }
   ],
   "source": [
    "print('# duplicate sets', len(linked_records))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write our original data back out to a CSV with a new column called \n",
    "# 'Cluster ID' which indicates which records refer to each other.\n",
    "\n",
    "cluster_membership = {}\n",
    "cluster_id = None\n",
    "for cluster_id, (cluster, score) in enumerate(linked_records):\n",
    "    for record_id in cluster:\n",
    "        cluster_membership[record_id] = (cluster_id, score)\n",
    "\n",
    "if cluster_id :\n",
    "    unique_id = cluster_id + 1\n",
    "else :\n",
    "    unique_id =0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(output_file, 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    \n",
    "    header_unwritten = True\n",
    "\n",
    "    for fileno, filename in enumerate(('addresses.csv', 'user_entries.csv')) :\n",
    "        with open(filename) as f_input :\n",
    "            reader = csv.reader(f_input)\n",
    "\n",
    "            if header_unwritten :\n",
    "                heading_row = next(reader)\n",
    "                heading_row.insert(0, 'source file')\n",
    "                heading_row.insert(0, 'Link Score')\n",
    "                heading_row.insert(0, 'Cluster ID')\n",
    "                writer.writerow(heading_row)\n",
    "                header_unwritten = False\n",
    "            else :\n",
    "                next(reader)\n",
    "\n",
    "            for row_id, row in enumerate(reader):\n",
    "                cluster_details = cluster_membership.get(filename + str(row_id))\n",
    "                if cluster_details is None :\n",
    "                    cluster_id = unique_id\n",
    "                    unique_id += 1\n",
    "                    score = None\n",
    "                else :\n",
    "                    cluster_id, score = cluster_details\n",
    "                row.insert(0, fileno)\n",
    "                row.insert(0, score)\n",
    "                row.insert(0, cluster_id)\n",
    "                writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the street address: \n",
      "Enter the city: \n",
      "Enter the state: \n",
      "Enter the zip code: \n"
     ]
    }
   ],
   "source": [
    "addr = input('Enter the street address: ')\n",
    "city = input('Enter the city: ')\n",
    "state = input('Enter the state: ')\n",
    "zip_code = input('Enter the zip code: ')\n",
    "\n",
    "# create a new file that will contain the user's entry\n",
    "user_input_file = 'user_input_file.csv'\n",
    "with open (user_input_file, 'w', newline='') as csvfile:\n",
    "    fieldnames = ['unique_id','title', 'description']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    writer.writerow({'unique_id' : \"1\", 'title': addr, 'description': city +\" \" + state + \" \" + \" \" + zip_code})\n",
    "csvfile.close()\n",
    "   \n",
    "# Actual address is 777 Brockton Avenue Abington MA 2351"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fields = [{'field' : 'title', 'type': 'String'},\n",
    "#         {'field' : 'title', 'type': 'Address'},{'field':'description', 'type':'String'},{'field':'description', 'type':'Address'}]\n",
    "    \n",
    "# duper = dedupe.RecordLink(fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_entry = readData('user_input_file.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# duper.sample(data_1,data_entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# dedupe.consoleLabel(duper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now, need to update the settings file and training file when user enters a new address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:dedupe.api:0 records\n",
      "INFO:dedupe.api:100 records\n",
      "INFO:dedupe.api:200 records\n"
     ]
    },
    {
     "ename": "BlockingError",
     "evalue": "No records have been blocked together. Is the data you are trying to match like the data you trained on?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBlockingError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-2614bbcb8270>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlinker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_entry\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/dedupe/api.py\u001b[0m in \u001b[0;36mmatch\u001b[0;34m(self, data_1, data_2, threshold, generator)\u001b[0m\n\u001b[1;32m    373\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mclusters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 375\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclusters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pragma: no cover\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/dedupe/api.py\u001b[0m in \u001b[0;36mmatchBlocks\u001b[0;34m(self, blocks, threshold, *args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m                                        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m                                        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_cores\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m                                        threshold=0)\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"matching done, begin clustering\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/dedupe/core.py\u001b[0m in \u001b[0;36mscoreDuplicates\u001b[0;34m(records, data_model, classifier, num_cores, threshold)\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0mfirst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpeek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfirst\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m         raise BlockingError(\"No records have been blocked together. \"\n\u001b[0m\u001b[1;32m    216\u001b[0m                             \u001b[0;34m\"Is the data you are trying to match like \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m                             \"the data you trained on?\")\n",
      "\u001b[0;31mBlockingError\u001b[0m: No records have been blocked together. Is the data you are trying to match like the data you trained on?"
     ]
    }
   ],
   "source": [
    "linker.match(data_1, data_entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "addr = input('Enter the street address: ')\n",
    "city = input('Enter the city: ')\n",
    "state = input('Enter the state: ')\n",
    "zip_code = input('Enter the zip code: ')\n",
    "\n",
    "# create a new file that will contain the user's entry\n",
    "user_input_file = 'user_input_file2.csv'\n",
    "with open (user_input_file, 'w', newline='') as csvfile:\n",
    "    fieldnames = ['unique_id','title', 'description']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    writer.writerow({'unique_id' : \"1\", 'title': addr, 'description': city +\" \" + state + \" \" + \" \" + zip_code})\n",
    "csvfile.close()\n",
    "   \n",
    "# Actual address is 777 Brockton Avenue Abington MA 2351"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_entry = readData('user_input_file2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:dedupe.api:0 records\n",
      "INFO:dedupe.api:100 records\n",
      "INFO:dedupe.api:200 records\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(('addresses.csv0', 'user_input_file2.csv0'), 0.99990386)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linker.match(data_1, data_entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the street address: \n",
      "Enter the city: \n",
      "Enter the state: \n",
      "Enter the zip code: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:dedupe.api:0 records\n",
      "INFO:dedupe.api:100 records\n",
      "INFO:dedupe.api:200 records\n"
     ]
    },
    {
     "ename": "BlockingError",
     "evalue": "No records have been blocked together. Is the data you are trying to match like the data you trained on?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBlockingError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-eda4e90b5306>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mdata_entry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreadData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'user_input_file3.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mlinker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_entry\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/dedupe/api.py\u001b[0m in \u001b[0;36mmatch\u001b[0;34m(self, data_1, data_2, threshold, generator)\u001b[0m\n\u001b[1;32m    373\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mclusters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 375\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclusters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pragma: no cover\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/dedupe/api.py\u001b[0m in \u001b[0;36mmatchBlocks\u001b[0;34m(self, blocks, threshold, *args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m                                        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m                                        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_cores\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m                                        threshold=0)\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"matching done, begin clustering\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/dedupe/core.py\u001b[0m in \u001b[0;36mscoreDuplicates\u001b[0;34m(records, data_model, classifier, num_cores, threshold)\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0mfirst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpeek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfirst\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m         raise BlockingError(\"No records have been blocked together. \"\n\u001b[0m\u001b[1;32m    216\u001b[0m                             \u001b[0;34m\"Is the data you are trying to match like \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m                             \"the data you trained on?\")\n",
      "\u001b[0;31mBlockingError\u001b[0m: No records have been blocked together. Is the data you are trying to match like the data you trained on?"
     ]
    }
   ],
   "source": [
    "addr = input('Enter the street address: ')\n",
    "city = input('Enter the city: ')\n",
    "state = input('Enter the state: ')\n",
    "zip_code = input('Enter the zip code: ')\n",
    "\n",
    "# create a new file that will contain the user's entry\n",
    "user_input_file = 'user_input_file3.csv'\n",
    "with open (user_input_file, 'w', newline='') as csvfile:\n",
    "    fieldnames = ['unique_id','title', 'description']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    writer.writerow({'unique_id' : \"1\", 'title': addr, 'description': city +\" \" + state + \" \" + \" \" + zip_code})\n",
    "csvfile.close()\n",
    "   \n",
    "# Actual address is 777 Brockton Avenue Abington MA 2351\n",
    "\n",
    "data_entry = readData('user_input_file3.csv')\n",
    "\n",
    "linker.match(data_1, data_entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the street address: \n",
      "Enter the city: \n",
      "Enter the state: \n",
      "Enter the zip code: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:dedupe.api:0 records\n",
      "INFO:dedupe.api:100 records\n",
      "INFO:dedupe.api:200 records\n"
     ]
    },
    {
     "ename": "BlockingError",
     "evalue": "No records have been blocked together. Is the data you are trying to match like the data you trained on?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBlockingError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-268c8db317ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mdata_entry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreadData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'user_input_file4.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mlinker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_entry\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/dedupe/api.py\u001b[0m in \u001b[0;36mmatch\u001b[0;34m(self, data_1, data_2, threshold, generator)\u001b[0m\n\u001b[1;32m    373\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mclusters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 375\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclusters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pragma: no cover\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/dedupe/api.py\u001b[0m in \u001b[0;36mmatchBlocks\u001b[0;34m(self, blocks, threshold, *args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m                                        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m                                        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_cores\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m                                        threshold=0)\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"matching done, begin clustering\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/dedupe/core.py\u001b[0m in \u001b[0;36mscoreDuplicates\u001b[0;34m(records, data_model, classifier, num_cores, threshold)\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0mfirst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpeek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfirst\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m         raise BlockingError(\"No records have been blocked together. \"\n\u001b[0m\u001b[1;32m    216\u001b[0m                             \u001b[0;34m\"Is the data you are trying to match like \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m                             \"the data you trained on?\")\n",
      "\u001b[0;31mBlockingError\u001b[0m: No records have been blocked together. Is the data you are trying to match like the data you trained on?"
     ]
    }
   ],
   "source": [
    "addr = input('Enter the street address: ')\n",
    "city = input('Enter the city: ')\n",
    "state = input('Enter the state: ')\n",
    "zip_code = input('Enter the zip code: ')\n",
    "\n",
    "# create a new file that will contain the user's entry\n",
    "user_input_file = 'user_input_file4.csv'\n",
    "with open (user_input_file, 'w', newline='') as csvfile:\n",
    "    fieldnames = ['unique_id','title', 'description']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    writer.writerow({'unique_id' : \"1\", 'title': addr, 'description': city +\" \" + state + \" \" + \" \" + zip_code})\n",
    "csvfile.close()\n",
    "   \n",
    "# Actual address is 777 Brockton Avenue Abington MA 2351\n",
    "\n",
    "data_entry = readData('user_input_file4.csv')\n",
    "\n",
    "linker.match(data_1, data_entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "file must have a 'write' attribute",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-3c058c446547>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Save the record linkage object to a pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"model_obj.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: file must have a 'write' attribute"
     ]
    }
   ],
   "source": [
    "# Save the record linkage object to a pickle\n",
    "import pickle\n",
    "pickle.dump(linker, \"model_obj\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
